# ML Engine Configuration
ml_engine:
  model_type: "isolation_forest"  # Options: isolation_forest, autoencoder, one_class_svm
  model_path: "models/anomaly_detector.joblib"
  training:
    contamination: 0.1  # Expected proportion of anomalies in the data
    random_state: 42
    n_estimators: 100
  prediction:
    threshold: 0.95  # Confidence threshold for anomaly detection
    batch_size: 1000

# Feature Extraction Configuration
feature_extraction:
  text_features:
    max_features: 100
    stop_words: "english"
    ngram_range: [1, 2]
  sequence_features:
    window_size: 10
    max_sequence_length: 100
  correlation_features:
    time_window: "1h"
    min_correlation: 0.3
  entropy_features:
    window_size: 50
    min_entropy: 0.1

# Wazuh ML Integration
wazuh_ml:
  enabled: true
  api_url: "https://wazuh-manager:55000"
  api_user: "wazuh-wui"
  api_password: ""  # Set via environment variable
  verify_ssl: true
  timeout: 30

# Notification Settings
notifications:
  slack:
    enabled: true
    webhook_url: ""  # Set via environment variable
    channel: "#security-alerts"
    username: "AluhaSIEM"
    icon_emoji: ":warning:"
  email:
    enabled: false
    smtp_server: "smtp.gmail.com"
    smtp_port: 587
    sender_email: ""
    sender_password: ""  # Set via environment variable
    recipients: []

# Incident Management
incident_management:
  rules:
    - name: "high_confidence_anomaly"
      condition: "confidence >= 0.95"
      severity: "high"
      auto_escalate: true
    - name: "medium_confidence_anomaly"
      condition: "confidence >= 0.8"
      severity: "medium"
      auto_escalate: false
    - name: "low_confidence_anomaly"
      condition: "confidence >= 0.6"
      severity: "low"
      auto_escalate: false
  escalation:
    max_retries: 3
    retry_interval: 300  # seconds
    escalation_timeout: 3600  # seconds

# Logging Configuration
logging:
  level: "INFO"
  format: "json"
  file: "logs/aluha_siem.log"
  max_size: 10485760  # 10MB
  backup_count: 5

# API Configuration
api:
  host: "0.0.0.0"
  port: 5000
  debug: false
  workers: 4
  timeout: 30
  rate_limit: 100  # requests per minute 